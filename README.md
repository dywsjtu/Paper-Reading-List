# Introduction

I include some interesting papers I've read. 

## GPU Cluster Management

[OSDI '21] Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning

## Framework
[OSDI '20] Retiarii: A Deep Learning Exploratory-Training Framework 

[VLDB '20] PyTorch Distributed: Experiences on Accelerating Data Parallel Training

[OSDI '18] Ray: A Distributed Framework for Emerging AI Applications

## Distributed System & Parallelization
[OSDI '22] Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization

## Scheduling 
[EuroSys '22] Out-Of-Order BackProp: An Effective Scheduling Technique for Deep Learning

[ATC '21] Zico: Efficient GPU Memory Sharing for Concurrent DNN Training

[OSDI '20] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications

[MLSys '20] Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications
