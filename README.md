# Introduction

I include some interesting papers I've read. 

## GPU Cluster Management

[OSDI '21] Pollux: Co-adaptive Cluster Scheduling for Goodput-Optimized Deep Learning

[OSDI '20] Heterogeneity-aware cluster scheduling policies for deep learning workloads

[OSDI '20] Antman: Dynamic scaling on GPU clusters for deep learning.

[NSDI '20] Themis: Fair and Efficient GPU Cluster Scheduling

[NSDI '19] Tiresias: A GPU Cluster Manager for Distributed Deep Learning

[OSDI '18] Gandiva: Introspective Cluster Scheduling for Deep Learning

[EuroSys '18] Optimus: An Efficient Dynamic Resource Scheduler for Deep Learning Clusters

## Framework
[OSDI '20] Retiarii: A Deep Learning Exploratory-Training Framework 

[VLDB '20] PyTorch Distributed: Experiences on Accelerating Data Parallel Training

[OSDI '18] Ray: A Distributed Framework for Emerging AI Applications

## Distributed System & Parallelization
[OSDI '22] Unity: Accelerating DNN Training Through Joint Optimization of Algebraic Transformations and Parallelization

## Scheduling 
[EuroSys '22] Out-Of-Order BackProp: An Effective Scheduling Technique for Deep Learning

[ATC '21] Zico: Efficient GPU Memory Sharing for Concurrent DNN Training

[OSDI '20] PipeSwitch: Fast Pipelined Context Switching for Deep Learning Applications

[MLSys '20] Salus: Fine-Grained GPU Sharing Primitives for Deep Learning Applications
